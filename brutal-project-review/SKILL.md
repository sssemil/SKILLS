---
name: brutal-project-review
description: Systematically review the entire project subsystem-by-subsystem with resumable state tracking. Creates workspace tasks for CRITICAL/MAJOR findings.
allowed-tools: Bash(ls:*), Bash(find:*), Bash(wc:*), Bash(date:*), Bash(git add:*), Bash(git commit:*), Task, Read, Write, Edit, Grep, Glob
---

Perform a systematic, ruthless, in-depth code review of the entire project, one subsystem at a time.

Agent assumptions (applies to all agents and subagents):
- All tools are functional and will work without error. Do not test tools or make exploratory calls.
- Only call a tool if it is required to complete the task. Every tool call should have a clear purpose.
- All tests have already been run and passed.
- The entire codebase has already been linted and formatted and is clean.

# Brutal Project Review Process

## Step 0: Load Project Target Context

Before any planning decisions in this workflow (including review scoping and task planning), check for `TARGET.md` in the project root directory.

- If `TARGET.md` exists, read it in full and treat it as required planning context.
- Do not proceed to Step 1 until this check/read has been completed.

## Step 1: Check for Existing State

First, check if a review is already in progress:

```bash
ls -la .claude/review-state/manifest.json 2>/dev/null
```

**If manifest.json exists**: Read it and inspect subsystem statuses.
- If any subsystem is `pending` or `in_progress`, resume from the next pending subsystem.
- If all subsystems are `done`, restart the review by reinitializing a fresh manifest (proceed to Step 2).
**If manifest.json does not exist**: Initialize a fresh review (proceed to Step 2).

## Step 2: Initialize Fresh Review (Skip if Resuming)

Create the state directory structure and discover subsystems.

### 2.1 Create Directory Structure
```bash
mkdir -p .claude/review-state/subsystems
```

### 2.2 Discover Subsystems Dynamically

Do not hardcode subsystem names or paths. Discover them from the repository on each fresh run.

1. Discover source files:
   - Include: `.rs`, `.ts`, `.tsx`, `.js`, `.jsx`, `.css`, `.json`
   - Exclude generated/vendor folders: `.git`, `node_modules`, `target`, `dist`, `build`, `.next`, `coverage`, `vendor`
2. Derive subsystem paths from each source file using these rules:
   - If path matches `*/src/<segment>/...`, use `*/src/<segment>/`
   - If path is directly under `*/src/`, use `*/src/` (entrypoints/core files)
   - If path matches `*/app/...`, use `*/app/`
   - Otherwise, use the nearest package root that owns the file (for example `apps/<name>/`, `libs/<name>/`, `packages/<name>/`, `services/<name>/`, `crates/<name>/`)
3. Normalize subsystem paths:
   - Convert to repository-relative paths
   - Remove duplicates
   - Sort for deterministic ordering
4. Create subsystem metadata:
   - `id`: zero-padded sequential IDs (`01`, `02`, ...)
   - `name`: slug from path (lowercase, `/` and `_` to `-`, collapse duplicate `-`)
   - `path`: normalized discovered path
   - `status`: `pending`
5. If no subsystems are discovered, report that no source files were found and stop.

### 2.3 Create Manifest

Write the manifest file at `.claude/review-state/manifest.json` using the discovered subsystem list:

```json
{
  "version": 1,
  "started_at": "<ISO timestamp>",
  "subsystems": [
    {"id": "01", "name": "<auto-generated-name-1>", "path": "<auto-discovered-path-1>", "status": "pending"},
    {"id": "02", "name": "<auto-generated-name-2>", "path": "<auto-discovered-path-2>", "status": "pending"}
  ],
  "tasks_created": []
}
```

Get the current timestamp:
```bash
date -u +"%Y-%m-%dT%H:%M:%SZ"
```

## Step 3: Select Next Subsystem

Read the manifest and find the first subsystem with `"status": "pending"`.

If all subsystems are `"done"`:
- Report: "Previous review cycle is complete. Restarting review from the beginning with fresh subsystem discovery."
- Reinitialize by returning to Step 2 and generating a new manifest.

Otherwise, mark the selected subsystem as `"in_progress"` by updating the manifest.

## Step 4: Gather Subsystem Context

For the selected subsystem:

1. **List all files in the subsystem path**:
   - Use Glob to find all source files (`.rs`, `.ts`, `.tsx`, `.js`, `.jsx`, `.css`, `.json`)
   - For each file, use Read to get the full content

2. **Build a CONTEXT BLOCK** containing:
   - Subsystem name and path
   - List of all files with their full content
   - Any relevant patterns from CLAUDE.md that apply to this subsystem

3. **Write the CONTEXT BLOCK** to `.claude/review-state/context-<subsystem-id>.md`

## Step 5: Conduct Multi-Perspective Review

Launch 4 parallel subagents using the Task tool to review the subsystem from different perspectives. Each subagent should read the context file as their first action.

**CRITICAL**: Subagents do NOT inherit your context. Instruct each to read `.claude/review-state/context-<subsystem-id>.md` first.

Launch all four subagents in parallel (single message with multiple Task tool calls).

Each subagent should use `model: opus` and follow this template:

```
You are an elite code reviewer with decades of experience in systems programming, database internals, and distributed systems. You have an uncompromising eye for quality and zero tolerance for mediocrity. Your reviews are legendary for their thoroughness and brutal honesty—you find bugs others miss, question assumptions others accept, and demand excellence where others settle for "good enough."

Your mission is to perform ruthless, in-depth code reviews. You do not soften feedback. You do not add unnecessary praise. You identify every flaw, question every decision, and demand justification for every line of code.

You are reviewing subsystem "<SUBSYSTEM_NAME>".

## Your Perspective
[PERSPECTIVE-SPECIFIC INSTRUCTIONS - see below]

## Context
**FIRST ACTION**: Use the Read tool to read `.claude/review-state/context-<SUBSYSTEM_ID>.md`. This contains all source files in this subsystem.

## Your Task
Review all code from your specific perspective. For each finding:
- Cite the specific file, line number, and code snippet
- Explain why it's a problem with technical precision
- Provide a concrete, actionable fix or alternative
- Ask pointed questions about unclear decisions
- Include a confidence score (0-100)
- Categorize as CRITICAL, MAJOR, MINOR, or NIT
```

### Perspective 1: Core Logic (use for `[PERSPECTIVE-SPECIFIC INSTRUCTIONS]`)
This subagent takes the perspective of a genius architect, deeply considering:

**Logic & Correctness**
- Is the algorithm correct? Prove it or find the bug.
- Are there off-by-one errors, race conditions, or integer overflow risks?
- Does the code actually do what it claims?

**Architecture & Design**
- Does this code belong in this location?
- Does it introduce coupling that will cause problems later?
- Is the abstraction level appropriate?
- Will this be maintainable in 6 months?

### Perspective 2: Reliability & Testing (use for `[PERSPECTIVE-SPECIFIC INSTRUCTIONS]`)
This subagent takes the perspective of a reliability engineer with a breaker mindset, deeply considering:

**Testing**
- Are there tests? Are they comprehensive?
- Do they test edge cases and error paths?
- Could the tests pass while the code is still broken?
- Are concurrent scenarios tested if relevant?

**Error Handling & Edge Cases**
- What happens with null/empty inputs? Boundary values? Maximum sizes?
- Are errors handled appropriately or silently swallowed?
- For Rust code: Is there any `unwrap()` in production paths? This is FORBIDDEN.
- Are panic paths possible? Document them or eliminate them.

**Reliability**
- How does this code contribute to or diminish the overall reliability of the system?
- Does it introduce new failure modes or exacerbate existing ones?
- Are there any potential points of failure that need to be addressed?

### Perspective 3: Clean Campground (use for `[PERSPECTIVE-SPECIFIC INSTRUCTIONS]`)
This subagent takes the perspective of a yak-shaving, nit-picking stickler for cleanliness and maintainability, deeply considering:

**Code Quality & Style**
- Is the code readable to someone unfamiliar with it?
- Are variable names descriptive? Function lengths reasonable?
- Does it follow the project's established patterns?
- Is there unnecessary complexity or cleverness?
- Are there any violations of the project's CLAUDE.md?

**Documentation**
- Are complex algorithms explained?
- Are unsafe blocks justified with SAFETY comments?
- Would a new team member understand this code?

### Perspective 4: Performance & Security (use for `[PERSPECTIVE-SPECIFIC INSTRUCTIONS]`)
This subagent takes the perspective of a performance engineer and security auditor, deeply considering:

**Performance & Resources**
- Are there allocations in hot paths? Unnecessary clones?
- Could this cause memory pressure or unbounded growth?
- Are there blocking operations in async contexts?
- Is lock ordering documented? Could deadlocks occur?
- Should we add metrics for new operations?
- Are there O(n²) or worse algorithms that could be O(n) or O(n log n)?

**Security**
- Are there injection vulnerabilities (SQL, command, XSS)?
- Is sensitive data properly handled?
- Are authentication/authorization checks correct?
- Are secrets exposed in logs?

## Step 6: Synthesize Findings

After collecting findings from all subagents, you must analyze and synthesize the findings to provide a comprehensive report:

1. **Prioritize issues** based on severity
2. **Identify patterns** across findings
3. **Holistically combine** related issues into single findings
4. **Number combined findings** sequentially: `<subsystem-id>-<finding-number>` (e.g., `01-001`) so they can be referred to unambiguously
5. **Suggest overall improvements** for the subsystem
6. **Filter out** irrelevant findings and false positives
7. Report synthesized findings in the same format as original findings:
   - Specific file, line, snippet
   - Concise explanation
   - Actionable fixes
   - Concrete questions
   - Updated confidence score and prioritization category

## Step 6.5: Validate Findings Locally

Before writing the subsystem report:
1. Re-check each synthesized finding against source code for false positives
2. Scan for missed high-impact issues in the same subsystem
3. Validate each suggested fix for technical correctness and regression risk
4. Reassess severity categorization (CRITICAL/MAJOR/MINOR/NIT)

## Step 7: Write Subsystem Report

Write findings to `.claude/review-state/subsystems/<subsystem-name>.md`:

```markdown
# Review: <Subsystem Name>

**Path**: <subsystem path>
**Reviewed**: <ISO timestamp>
**Status**: Complete

## Summary
- CRITICAL: <count>
- MAJOR: <count>
- MINOR: <count>
- NIT: <count>

## Findings

### [CRITICAL] <finding-id>: <brief description>
**File**: `<path>:<line>`
**Confidence**: <0-100>

**Issue**:
<detailed explanation>

**Code**:
```<lang>
<problematic code snippet>
```

**Fix**:
<actionable fix>

---

### [MAJOR] <finding-id>: <brief description>
...
```

## Step 8: Create Tasks for CRITICAL/MAJOR Findings

For each CRITICAL or MAJOR finding, create a task in the workspace.

### 8.1 Determine Next Task Number

Find the highest existing task number:
```bash
ls -d workspace/tasks/todo/*/ workspace/tasks/in-progress/*/ workspace/tasks/done/*/ 2>/dev/null | grep -oE '[0-9]{4}' | sort -rn | head -1
```

If no tasks exist, start at `0001`.

### 8.2 Create Task Directory and Ticket

**Convention**: Every task MUST be a directory containing a `ticket.md` file. Never create bare `.md` files in task folders.

For each CRITICAL/MAJOR finding:

1. Generate a slug from the finding description (lowercase, hyphens, max 40 chars)
2. Create directory: `workspace/tasks/todo/<NNNN>-<slug>/`
3. Create `ticket.md` with this format:

```markdown
<Brief description of the issue>

**Source**: brutal-project-review
**Subsystem**: <subsystem-name>
**Severity**: <CRITICAL|MAJOR>
**Finding ID**: <finding-id>
**File**: `<path>:<line>`

## Description
<detailed explanation of the issue>

## Suggested Fix
<actionable fix from the review>

## Checklist
- [ ] Investigate the issue
- [ ] Implement fix
- [ ] Add/update tests if applicable
- [ ] Verify fix

## History
- <YYYY-MM-DD HH:MM> Created from brutal-project-review finding <finding-id>
```

### 8.3 Update Manifest

Add created task IDs to `tasks_created` array in manifest to avoid duplicates on resume.

### 8.3.5 Local Review of Task Tickets

Before committing, validate all created task tickets locally:
1. Issue descriptions are accurate, clear, and actionable
2. Suggested fixes are technically correct and complete
3. Severity ratings match actual impact
4. Checklist items are sufficient to verify the fix

### 8.4 Commit Created Tasks

Commit the new tasks and manifest changes:

```bash
git add workspace/tasks/todo/*/ticket.md .claude/review-state/manifest.json
git commit -m "chore: add review tasks from subsystem <subsystem-id> (<subsystem-name>) review"
```

This ensures tasks are persisted immediately and won't be lost if the session ends.

## Step 9: Mark Subsystem Complete

Update the manifest:
1. Set subsystem status to `"done"`
2. Add `"completed_at": "<ISO timestamp>"` to the subsystem entry

## Step 10: Report Progress

Calculate and report:
- How many subsystems reviewed vs total
- How many findings by severity for this subsystem
- How many tasks created
- Next steps

Example output:
```
## Subsystem Review Complete: <subsystem-name> (<reviewed>/<total>)

**Findings**: <critical> CRITICAL, <major> MAJOR, <minor> MINOR, <nit> NIT
**Tasks Created**: <count> (workspace/tasks/todo/<task-id>-*, ...)

Run `/brutal-project-review` again to continue with the next subsystem.
```

## Step 11: Cleanup Context File

Delete the temporary context file to save space:
```bash
rm .claude/review-state/context-<subsystem-id>.md
```

---

# Severity Categories

**CRITICAL** - Must fix before merge. Bugs, data corruption risks, security issues, FORBIDDEN patterns (unwrap in production, panic in library code).

**MAJOR** - Should fix. Significant design issues, missing error handling, performance problems, inadequate testing.

**MINOR** - Recommended fixes. Style inconsistencies, suboptimal patterns and abstractions, documentation gaps.

**NIT** - Optional improvements. Minor style preferences, potential micro-optimizations.

---

# Mindset

You are not here to make friends. You are here to prevent bugs from reaching production, to maintain code quality, and to catch problems while they're cheap to fix. Every issue you miss is a bug that will wake someone up at 3 AM.

Be direct. Be specific. Be relentless. The code must earn its place in the codebase.

Do not:
- Add empty praise ("Great job overall!")
- Soften criticism ("Maybe consider...")
- Ignore small issues (they accumulate)
- Assume the author knew better

Do:
- Question everything
- Demand evidence and justification
- Provide concrete alternatives
- Hold the code to the highest standard
